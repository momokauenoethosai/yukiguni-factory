import csv
import os
import time
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from dotenv import load_dotenv
import google.generativeai as genai
from datetime import datetime
import pytz

load_dotenv()

def parse_flyer_info(text: str) -> tuple[str, str]:
    """ãƒãƒ©ã‚·æƒ…å ±ã‚’æœŸé–“ã¨ã‚¿ã‚¤ãƒˆãƒ«ã«åˆ†é›¢
    Returns: (flyer_title, period)
    """
    if not text or not text.strip():
        return text, ""

    # æœŸé–“ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºï¼ˆä¾‹: 11/10ï½11/16, 11/1ï½11/30, 10/1ï½10/31ï¼‰
    period_patterns = [
        r'(\d{1,2}/\d{1,2}\s*ï½\s*\d{1,2}/\d{1,2})',  # 11/10ï½11/16
        r'(\d{1,2}/\d{1,2}\s*ï½\s*\d{1,2}/\d{1,2})',  # 11/1ï½11/30 (ã‚¹ãƒšãƒ¼ã‚¹ä»˜ã)
        r'(\d{1,2}/\d{1,2}\s*-\s*\d{1,2}/\d{1,2})',   # ãƒã‚¤ãƒ•ãƒ³åŒºåˆ‡ã‚Š
        r'(\d{4}/\d{1,2}/\d{1,2}\s*ï½\s*\d{4}/\d{1,2}/\d{1,2})',  # å¹´ä»˜ã
    ]

    for pattern in period_patterns:
        match = re.search(pattern, text)
        if match:
            period = match.group(1).strip()
            # æ®‹ã‚Šã®éƒ¨åˆ†ã‚’ã‚¿ã‚¤ãƒˆãƒ«ã¨ã™ã‚‹
            flyer_title = text.replace(match.group(0), "").strip()

            # æœŸé–“ã®ã¿ã®å ´åˆã¯ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç©ºã«ã™ã‚‹
            if not flyer_title or flyer_title == period:
                return "", period
            else:
                return flyer_title, period

    # æœŸé–“ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…¨ã¦ã‚¿ã‚¤ãƒˆãƒ«ã¨ã™ã‚‹
    return text, ""

def quick_keyword_filter(title: str) -> str:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§ç´ æ—©ã„åˆ¤å®šã‚’è¡Œã†
    Returns: 'FOOD', 'NON_FOOD', 'UNKNOWN'
    """
    if not title or not title.strip():
        return 'UNKNOWN'

    title_lower = title.lower()

    # æ˜ç¢ºã«é£Ÿå“é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    food_keywords = [
        'ãƒ‘ãƒ³', 'ã”ã¡ãã†', 'ã‚°ãƒ«ãƒ¡', 'å¤œå¸‚', 'é£Ÿ', 'è‚‰', 'é­š', 'é‡èœ', 'æœç‰©',
        'æƒ£èœ', 'ç”Ÿé®®', 'ã‚¿ã‚¤ãƒ ã‚»ãƒ¼ãƒ«', 'ãŠã›ã¡', 'ã‚±ãƒ¼ã‚­', 'åˆºèº«', 'å¼å½“',
        'ãƒ‡ãƒªã‚«', 'ãƒ™ãƒ¼ã‚«ãƒªãƒ¼', 'é…’', 'ãƒ“ãƒ¼ãƒ«', 'å®šæœŸä¾¿', 'äºˆç´„æ‰¿ã‚Š'
    ]

    # æ˜ç¢ºã«éé£Ÿå“ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    non_food_keywords = [
        'ã‚³ã‚¹ãƒ¡', 'åŒ–ç²§å“', 'è¡£æ–™', 'ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³', 'å®¶é›»', 'é›‘è²¨', 'æ–‡å…·',
        'ã‚¤ãƒ³ãƒ†ãƒªã‚¢', 'ã‚µã‚¤ã‚¯ãƒ«', 'è‡ªè»¢è»Š', 'ãŠã‚‚ã¡ã‚ƒ', 'ç©å…·', 'æœ¬', 'æ›¸ç±',
        'CD', 'DVD', 'ã‚²ãƒ¼ãƒ ', 'ã‚¹ãƒãƒ¼ãƒ„', 'é´', 'ãƒãƒƒã‚°', 'æ™‚è¨ˆ', 'ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼'
    ]

    # éé£Ÿå“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
    for keyword in non_food_keywords:
        if keyword in title:
            return 'NON_FOOD'

    # é£Ÿå“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
    for keyword in food_keywords:
        if keyword in title:
            return 'FOOD'

    # æœŸé–“è¡¨è¨˜ã®ã¿ã®å ´åˆã¯é£Ÿå“æ‰±ã„ï¼ˆä¸€èˆ¬çš„ãªã‚¿ã‚¤ãƒ ã‚»ãƒ¼ãƒ«ãªã©ï¼‰
    if '/' in title and 'ï½' in title:
        return 'FOOD'

    return 'UNKNOWN'

def is_food_flyer_by_title(title: str) -> bool:
    """2æ®µéšåˆ¤å®šã§ãƒãƒ©ã‚·ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é£Ÿå“ãƒãƒ©ã‚·ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    if not title or not title.strip():
        return True  # ã‚¿ã‚¤ãƒˆãƒ«ãŒç©ºã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å«ã‚ã‚‹

    # ç¬¬1æ®µéš: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®é«˜é€Ÿåˆ¤å®š
    keyword_result = quick_keyword_filter(title)

    if keyword_result == 'FOOD':
        print(f"Keyword filtering: '{title}' -> FOOD (confirmed)")
        return True
    elif keyword_result == 'NON_FOOD':
        print(f"Keyword filtering: '{title}' -> NON-FOOD (confirmed)")
        return False

    # ç¬¬2æ®µéš: åˆ¤å®šãŒæ›–æ˜§ãªå ´åˆã®ã¿Gemini APIã‚’ä½¿ç”¨
    print(f"Keyword filtering: '{title}' -> UNKNOWN, using Gemini...")

    try:
        # Streamlit Secrets ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
        try:
            import streamlit as st
            api_key = st.secrets.get("GOOGLE_GEMINI_API_KEY") or os.getenv('GOOGLE_GEMINI_API_KEY')
        except:
            api_key = os.getenv('GOOGLE_GEMINI_API_KEY')

        if not api_key:
            print("Warning: GOOGLE_GEMINI_API_KEY not found, defaulting to FOOD")
            return True

        # APIåˆ¶é™å¯¾ç­–: 6ç§’å¾…æ©Ÿ
        time.sleep(6)

        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-flash')

        prompt = f"""
        ä»¥ä¸‹ã®ãƒãƒ©ã‚·ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¦‹ã¦ã€ä¸»ã«é£Ÿå“ãƒ»é£²æ–™ãƒ»é£Ÿæã‚’æ‰±ã£ã¦ã„ã‚‹ãƒãƒ©ã‚·ã‹ã©ã†ã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

        ã‚¿ã‚¤ãƒˆãƒ«: "{title}"

        åˆ¤å®šåŸºæº–:
        - é£Ÿå“ãƒãƒ©ã‚·: é‡èœã€è‚‰ã€é­šã€æœç‰©ã€æƒ£èœã€ãƒ‘ãƒ³ã€é£²æ–™ã€é£Ÿæã€å¤œå¸‚ã€ã”ã¡ãã†ã€ã‚°ãƒ«ãƒ¡ãªã©é£Ÿå“é–¢é€£
        - éé£Ÿå“ãƒãƒ©ã‚·: åŒ–ç²§å“ã€ã‚³ã‚¹ãƒ¡ã€è¡£æ–™å“ã€ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ã€å®¶é›»ã€é›‘è²¨ã€æ–‡å…·ã€ã‚¤ãƒ³ãƒ†ãƒªã‚¢ãªã©

        å›ç­”ã¯ã€ŒYESã€ï¼ˆé£Ÿå“ãƒãƒ©ã‚·ï¼‰ã¾ãŸã¯ã€ŒNOã€ï¼ˆéé£Ÿå“ãƒãƒ©ã‚·ï¼‰ã®ã¿ã§ãŠç­”ãˆãã ã•ã„ã€‚
        æœŸé–“ã®ã¿ã®è¡¨è¨˜ï¼ˆä¾‹ï¼š10/1ï½10/31ï¼‰ã®å ´åˆã¯ã€ŒYESã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚
        """

        response = model.generate_content(prompt)
        result = "YES" in response.text.upper()
        print(f"Gemini filtering: '{title}' -> {'FOOD' if result else 'NON-FOOD'}")
        return result

    except Exception as e:
        print(f"Error in Gemini filtering for '{title}': {e}")
        return True  # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å«ã‚ã‚‹

def scrape_chirashi_data_selenium(input_csv, output_csv):
    results = []
    total_items = 0
    filtered_items = 0
    # æ—¥æœ¬æ™‚é–“ã§ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç”Ÿæˆ
    jst = pytz.timezone('Asia/Tokyo')
    scraped_at = datetime.now(jst).strftime('%Y-%m-%d %H:%M:%S')

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã‚’è¨­å®š
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    stop_flag_file = os.path.join(project_root, "temp_stop_flag.txt")

    # Chrome options for headless mode
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-plugins')

    try:
        print("ğŸš— ChromeDriverã‚’åˆæœŸåŒ–ä¸­...")
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        print("âœ… ChromeDriveråˆæœŸåŒ–å®Œäº†")
    except Exception as e:
        print(f"âŒ ChromeDriveråˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return

    try:
        with open(input_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # åœæ­¢ãƒ•ãƒ©ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
                if os.path.exists(stop_flag_file):
                    print("â¹ï¸ åœæ­¢è¦æ±‚ã‚’å—ä¿¡ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
                    break

                url = row['url']
                super_name = row['super_name']
                shop_name = row['shop_name']

                try:
                    print(f"Fetching URL: {url}")
                    driver.get(url)

                    # Wait for the flier_list to be populated
                    print("Waiting for content to load...")
                    wait = WebDriverWait(driver, 10)
                    wait.until(EC.presence_of_element_located((By.CLASS_NAME, "info-item")))

                    # Additional wait for all dynamic content
                    time.sleep(3)

                    # Get page source after JavaScript execution
                    page_source = driver.page_source

                    # Save HTML for inspection
                    html_filename = f"{shop_name}_selenium_response.html"
                    html_path = os.path.join(project_root, "output", html_filename)
                    with open(html_path, 'w', encoding='utf-8') as html_file:
                        html_file.write(page_source)
                    print(f"HTML saved to: {html_path}")

                    # Parse with BeautifulSoup
                    soup = BeautifulSoup(page_source, 'html.parser')

                    flier_list = soup.find('div', id='flier_list', class_='info-list')

                    if flier_list:
                        info_items = flier_list.find_all('div', class_='info-item')

                        for item in info_items:
                            link = item.find('a', class_='info-link')
                            if link:
                                href = link.get('href', '')
                                if href:
                                    full_url = urljoin(url, href)

                                    img = link.find('img', class_='info-item__img')
                                    img_src = ''
                                    if img:
                                        img_src = img.get('src', '')
                                        if img_src:
                                            img_src = urljoin(url, img_src)

                                    title_elem = link.find('h4', class_='info-item__title')
                                    raw_text = title_elem.text.strip() if title_elem else ''

                                    # ã‚¿ã‚¤ãƒˆãƒ«ã¨æœŸé–“ã‚’åˆ†é›¢
                                    flyer_title, period = parse_flyer_info(raw_text)

                                    total_items += 1

                                    # 2æ®µéšåˆ¤å®šã§ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é£Ÿå“ãƒãƒ©ã‚·ã‹ã©ã†ã‹ã‚’åˆ¤å®š
                                    print(f"ãƒãƒ©ã‚·URLå–å¾—: {raw_text} -> ã‚¿ã‚¤ãƒˆãƒ«:'{flyer_title}' æœŸé–“:'{period}'")
                                    if is_food_flyer_by_title(raw_text):
                                        results.append({
                                            'url': full_url,
                                            'super_name': super_name,
                                            'shop_name': shop_name,
                                            'chirashi_png_path': img_src,
                                            'flyer_title': flyer_title,
                                            'period': period,
                                            'scraped_at': scraped_at
                                        })
                                        filtered_items += 1
                                        print(f"âœ… é£Ÿå“ãƒãƒ©ã‚·ã¨ã—ã¦è¿½åŠ : {period}")
                                    else:
                                        print(f"âŒ éé£Ÿå“ãƒãƒ©ã‚·ã¨ã—ã¦ã‚¹ã‚­ãƒƒãƒ—: {period}")

                        print(f"Successfully scraped {len(info_items)} items from {shop_name}")
                    else:
                        print(f"No flier_list found for {shop_name}")

                except TimeoutException:
                    print(f"Timeout waiting for content to load for {shop_name}")
                except Exception as e:
                    print(f"Error processing {shop_name}: {e}")

    finally:
        driver.quit()

    # Save results to CSV
    with open(output_csv, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['url', 'super_name', 'shop_name', 'chirashi_png_path', 'flyer_title', 'period', 'scraped_at']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)

    print(f"\n=== Scraping Summary ===")
    print(f"Total items found: {total_items}")
    print(f"Food flyers (after filtering): {filtered_items}")
    print(f"Non-food flyers (filtered out): {total_items - filtered_items}")
    print(f"Results saved to: {output_csv}")

if __name__ == "__main__":
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã«å¤‰æ›´
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    input_path = os.path.join(project_root, "input", "super_list.csv")
    output_path = os.path.join(project_root, "output", "chirashi_data_selenium.csv")

    scrape_chirashi_data_selenium(input_path, output_path)